% arara: pdflatex: { synctex: yes }
% arara: makeindex: { style: ctuthesis }
% arara: bibtex

% The class takes all the key=value arguments that \ctusetup does,
% and a couple more: draft and oneside
\documentclass[twoside]{ctuthesis}

\ctusetup{
	preprint = \ctuverlog,
	mainlanguage = english,
%	titlelanguage = czech,
%	mainlanguage = czech,
	otherlanguages = {slovak, czech},
	title-czech = {Lokalizace a segmentace in-vivo ultrazvukových obrazů karotidy},
	title-english = {Localization and segmentation of in-vivo ultrasound carotid artery images},
%	subtitle-czech = {Cesta do tajů kdovíčeho},
%	subtitle-english = {Journey to the who-knows-what wondeland},
	doctype = M,
	faculty = F3,
	department-czech = {Katedra počítačů},
	department-english = {Department of Computer Science},
	author = {Martin Kostelanský},
	supervisor = {prof. Dr. Ing. Jan Kybic},
	supervisor-address = {Center for Machine Perception, \\ Karlovo náměstí 13, \\ Prague 2},
%	supervisor-specialist = {John Doe},
	fieldofstudy-english = {Open Informatics},
	subfieldofstudy-english = {Artificial Intelligence},
	fieldofstudy-czech = {Otevřená informatika},
	subfieldofstudy-czech = {Umělá inteligence},
	keywords-czech = {lekarsky ultrazvuk, stenoza karotid, konvolucna neuronova siet},
	keywords-english = {medcial ultrasound, carotid stenosis, convolutional neural network},
	day = 5,
	month = 1,
	year = 2021,
	specification-file = {zadanie.pdf},
%	front-specification = true,
%	front-list-of-figures = false,
%	front-list-of-tables = false,
%	monochrome = true,
%	layout-short = true,
}

\ctuprocess

\addto\ctucaptionsczech{%
	\def\supervisorname{Vedoucí}%
	\def\subfieldofstudyname{Studijní program}%
}

\ctutemplateset{maketitle twocolumn default}{
	\begin{twocolumnfrontmatterpage}
		\ctutemplate{twocolumn.thanks}
		\ctutemplate{twocolumn.declaration}
		\ctutemplate{twocolumn.abstract.in.titlelanguage}
		\ctutemplate{twocolumn.abstract.in.secondlanguage}
		\ctutemplate{twocolumn.tableofcontents}
		\ctutemplate{twocolumn.listoffigures}
	\end{twocolumnfrontmatterpage}
}

% Theorem declarations, this is the reasonable default, anybody can do what they wish.
% If you prefer theorems in italics rather than slanted, use \theoremstyle{plainit}
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{note}
\newtheorem*{remark*}{Remark}
\newtheorem{remark}[theorem]{Remark}

\setlength{\parskip}{5ex plus 0.2ex minus 0.2ex}

% Abstract in English
\begin{abstract-english}
	To write at the end.
		
\end{abstract-english}
	
% Abstract in Czech
\begin{abstract-czech}
To write at the end.

\end{abstract-czech}


% Acknowledgements / Podekovani
\begin{thanks}
I would like to thank to professor Kybic for his valuable counsel and answering my e-mails after 10 p.m.
Dedicated to my family and friends for their endless support during my studies.
\end{thanks}

% Declaration / Prohlaseni
\begin{declaration}
I declare that I have worked on this thesis independently using only the sources listed in the bibliography.

In Prague, \ctufield{day}.~\monthinlanguage{title}~\ctufield{year}
\end{declaration}

% Only for testing purposes
\listfiles
\usepackage[pagewise]{lineno}
\usepackage{lipsum,blindtext}
\usepackage{mathrsfs} % provides \mathscr used in the ridiculous examples

\begin{document}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
Artificial inteligence is a science field which aims to build intelligent systems
and understand the principles behind it \cite{AIMAbook}. Most of the researches
assumes that ability to learn is a predisposition for intelligence. \cite{KONONENKO200189}
Machine learning is a subfield of AI, which focuses on learning a behavior from data.
The rise of deep learning, an area of machine learning which focuses on deep neural networks,
started around 2010, when this models outperformed algorithm based methods \cite{DL_BOOK}. 
Since than, deep learning has been applied in a wide range of applications, 
from natural language processing \cite{mikolov2013efficient}, finance \cite{Ding2015DeepLF}, 
image processing \cite{AlexNet} or medical diagnosis \cite{6868045}.

Paragraph - Image processing in medicine

Paragraph - Thesis - direct topis intro

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{kostema3_theoretical}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{kostema3_practical}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion and Results}
To write.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\chapter{Convolutional neural net}
Convolutional neural nets are a family of neural nets, which use a convolutional layer.
Usually they take as an input grid structured data \cite{DL_BOOK}, typicaly images, for example
 one may see as 3D tensor, one dimension for each primary color in RGB encoding. But from their
 first pracical use in reading check system \cite{CHECKS}, they have been applied in many 
 domains, not only necessarily image processing. They have been used in text processing, for
 example in anlayzing sentiment of a text \cite{SENTIMENT_CNN}, time series classification 
\cite{TS_CNN} or in the field of recommender systems \cite{REC_CNN}. Its main component is a
convolutional layer, which is often combined a pooling layer. One may find convolutional 
networks combined with fully-connected or even LSTM layers \cite{LSTM_CNN}. In this section
will be discussed the most used ones - convollutional, pooling and fully connected.


\section{Convolutional layer}
The cornerstone of each convolutional neural net is a convolutional layer. In this layer, 
one or multiple convolutional filters are applied on the layer's input. In the figure TODO we 
can see different convolutional filters applied on an image. In the neural network, each of the 
trainable filters is relatively small and serves as a feature extractor. At each position, 
filter's kernel takes an input from its receptors field, computes it's weighted sum, adds bias 
nd transform the input by a non-linar activation function. This is transformation described by 
the equation TODO add eq. The kernel is continuosly applied over an input, and the ditance
betweed two such operations is called stride. Since convolution reduce the dimension of an
image, the input is usully padded with some constant value (for example 0), to perserve it.
On the image TODO, it is shown convloution on an image of size 5x5, and the size of the 
kernel is 3x3. To keep the spatial dimension, zero padding is used.

\begin{equation} 
	Y_{i,j}=\sigma(b+\sum_{k=0}^{x}\sum_{l=0}^{y}w_{k,l}a_{i+k,j+l}),
\end{equation} 

\section{Pooling layer}
Pooling layers often follows the convolutional ones, and their purpose is to reduce 
the dimensionaluty. Fistly was used average pooling, which computed average value of
the receptor field, shown in equation A.3. In the last years was introduced, max-pooling, 
which propagates the maximum value at each postion \cite{CNNREW}. For example, using pooling
layer with receptor field of size 2x2 and stride 2, reduces dimension to falf. Such example
can be seen on fitgure TODO.

\begin{equation} 
	Y_{i,j}=\frac{1}{xy}\sum_{k=0}^{x}\sum_{l=0}^{y}w_{k,l}a_{i+k,j+l},
\end{equation} 

\begin{equation} 
	Y_{i,j} = \max_{(p,q)\in{\Re_{i,j}}}(x_{p,q}),
\end{equation} 


\section{Fully-connected layer}
Fully-connected layer is composed from one, or multiple neurons, where each neuron is connected
to every unit in previous layer by trainable weight. With added bias and transformation with
non-linear function, the output is forwarded to the next layer.

\begin{equation} 
	Y_{i,j}=\sigma(b +\sum_{k=0}^{n}w_{k,j}a_{k,j-1}),
\end{equation}


\section{Architecture}
Typicaly, input to the CNN is fixed, so the image needs to preprocessed accordingly.
Fistly, an input is proessed by series of convolutional layers grouped in blocks, 
where everu convolutional layer has the same same setting (number of filters, kernel size, 
stride,padding). The dimension between the block is reduced by a pooling layer, and the number
of filters in next conv. block increased. The last part is composed by one or multiple
fully-connected layers. The nubmer of neurons is depedent on the application. If network
should behave as a binary classifier, than we can use one neuron with sigmoid 
activation fcuntion. If the aim is to create classify n categories, the layer should
contain n neurons followed by a soft-max fucntion. In the case of localization of a single
object, four neurons can predict two corners of a box suronding the target. The whole Architecture
can be seen on figure TODO.


\section{Training}
During the training of a network, its free parametes, weights and biases, are being changed to values,
which make the model perform well on the training dataset. The performance of a model is meassured by a 
loss function. In the case of regression, sum-of-squared errors can be used to compute the fit

$$R\(\theta\) = finish$$.

To evaluate classifier can be used cross-entropy:

$$R\(\theta\) = finish$$.

The loss of a model is typicaly reduced by stochastic gradient descent, in a case of neural network also
called backpropagation - the gradient is propagated back trough the network. \cite{hastie2009elements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation details}
The project was implemented in programing language Python \cite{Python}, version 3.7.
For using, creating and training neural network is used deep learning library PyTorch\cite{PyTorch}.
Code documentation is following numpy docstring guide \cite{NP}, and it is formatted by Black code 
formater \cite{Black}


\section{Project structure}
The project is divided into three main parts: classification, localization and segmentation.
Code share between them is stored in the root directory. For each part there files which create
models, datasets and may contain some additional funcionality, for example data data augmentation.
The complete structure can be seen on figure TODO.

\section{Examples}
For every part there is an example script which traines sample model on specified dataset. For
every type of model there is a script with example usage as well as samle data on which these 
models can be ran. All of the examples scripts can be found the the root directory.


\chapter{List of abbreviations}
To write.

\printindex

\appendix

\bibliographystyle{amsalpha}
\bibliography{kostema3}

\ctutemplate{specification.as.chapter}

\end{document}
